{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Time Series \n",
    "\n",
    "Deep Learning is a subset of machine learning and excels when dealing with large and complex data, as it can extract complex features with minimal human involvement. Deep learning works well with structured and unstructured data and can be used in supervised, unsupervised and semi-supervised learning. \n",
    "\n",
    "This chapter focuses on using deep learning for time series forecasting - using different deep learning architectures suitable for sequential data such as time series data. There are different deep learning architectures for solving various problems: \n",
    "\n",
    "* Recurrent Neural Networks (RNNs)\n",
    "* Long-Short Term Memory (LSTM)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Convolutional Neural Networks (CNNs)\n",
    "* Autoencoders\n",
    "* Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardize:\n",
    "    def __init__(self, df, split=0.10):\n",
    "        self.data = df \n",
    "        self.split = split \n",
    "\n",
    "    def split_data(self):\n",
    "        n = int(len(self.data) * self.split)\n",
    "        train, test = self.data.iloc[:-n], self.data.iloc[-n:]\n",
    "        n = int(len(train) * self.split)\n",
    "        train, val = train.iloc[:-n], train.iloc[-n:]\n",
    "        assert len(test) + len(train) + len(val) == len(self.data)\n",
    "        return train, test, val \n",
    "    \n",
    "    def _transform(self, data):\n",
    "        data_s = (data - self.mu)/self.sigma \n",
    "        return data_s \n",
    "    \n",
    "    def fit_transform(self):\n",
    "        train, test, val = self.split_data()\n",
    "        self.mu, self.sigma = train.mean(), train.std()\n",
    "        train_s = self._transform(train)\n",
    "        test_s = self._transform(test)\n",
    "        val_s = self._transform(val)\n",
    "        return train_s, test_s, val_s \n",
    "    \n",
    "    def inverse(self, data):\n",
    "        return (data * self.sigma)+self.mu \n",
    "    \n",
    "    def inverse_y(self, data):\n",
    "        return (data * self.sigma[-1])+self.mu[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forecast(df, window):\n",
    "    d = df.values \n",
    "    x = []\n",
    "    n = len(df)\n",
    "    idx = df.index[:-window]\n",
    "    for start in range(n-window):\n",
    "        end = start + window\n",
    "        x.append(d[start:end])\n",
    "    cols = [f'x_{i}' for i in range(1, window+1)]\n",
    "    x = np.array(x).reshape(n-window, -1)\n",
    "    y = df.iloc[window:].values\n",
    "    df_xs = pd.DataFrame(x, columns=cols, index=idx)\n",
    "    df_y = pd.DataFrame(y.reshape(-1), columns=['y'], index=idx)\n",
    "\n",
    "    return pd.concat([df_y, df_xs], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "path = Path('../TimeSeriesAnalysisWithPythonCookbook/Data/')\n",
    "energy = pd.read_csv(path.joinpath('energy_consumption.csv'), index_col='Month', parse_dates=True)\n",
    "energy.columns = ['y']\n",
    "energy.index.freq = 'MS'\n",
    "en_df = one_step_forecast(energy, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using the updated Standardize class \n",
    "scale_en = Standardize(en_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, test_en, val_en = scale_en.fit_transform()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning libraries can be broken down into either **low-level, high-level** or both. High-level libraries allow for quick prototyping and experimentation when testing various architectures, such as the case with Keras. A low-level library gives us more flexibility and control, but we will have to define more aspects of a model's architecture - PyTorch and Tensorflow are examples of low-level libraries. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with an RNN using Keras \n",
    "\n",
    "RNNs initially entered the spotlight with NLP, as they were designed for sequential data, where past observations, such as words, have a strong influence on determining the next word in a sentence. This need for the artificial neural network to retain memory (hidden state) inspired the RNN architecture. Similarly, time series data is also sequential, and since past observations influece future observations, it also needs a network with memory. \n",
    "\n",
    "In RNNs, there is a feedback loop where the output of one node or neuron is fed back (the recursive part) as input, allowing the network to learn from a prior time step acting as a memory. \n",
    "\n",
    "In an RNN we have two outputs and two sets of weights: $W_{X}$ for the input and $W_{H}$ for the hidden state or memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\Projekter\\Statistics\\Training\\TimeSeriesAnalysisWithPythonCookbook\\Ch13.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/Projekter/Statistics/Training/TimeSeriesAnalysisWithPythonCookbook/Ch13.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/Projekter/Statistics/Training/TimeSeriesAnalysisWithPythonCookbook/Ch13.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mSequential\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "import tf.keras.Sequential \n",
    "\n",
    "# import Sequential \n",
    "#from tensorflow import keras \n",
    "# from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError \n",
    "# from keras.layers import Dense, SimpleRnn, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
